# llm_config.yaml
llm:
  provider: "ollama"
  model: "llama3.1:8b"
  lite_model: "llama3.1:8b"
  api_key: "${OPENAI_API_KEY}"
  base_url: "http://127.0.0.1:11434/v1"
  timeout: 60
  temperature: 0.2
  max_tokens: 1000
  retry_attempts: 2
  use_llama: true

  # LangChain Config
  langchain_api_key: "${LANGCHAIN_API_KEY}"
  langchain_project_name: "simulator_agent_dev"
  langchain_tracing: true
  
simulator:
  # Disable to save token usage
  disable_token_usage: false

agents:
  agent_validation:
    enabled: false
    regenerate_on_invalid: true

control_config:
  enable_realtime_log_summary: false
  enable_ai_feature: true

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

redis:
  host: "${REDIS_HOST}"
  port: ${REDIS_PORT}
  username: "${REDIS_USERNAME}"
  password: "${REDIS_PASSWORD}"
  db: 0
  ssl: true
  connection_timeout: 10

dev:
  enable_mock_responses: False